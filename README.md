# Image-Captioning-with-Visual-Attention
 The model tackles the task of automatically generating captions for images by focusing on different regions of the image as it generates each word in the caption.<br /> 
 A ResNet model is employed to encode the images, while LSTM cells with an attention mechanism serve as the decoder, generating the captions.<br /> 
 flicker8k dataset a dataset with 8,000 images and each with 5 different captions.
# Model

![model](https://github.com/user-attachments/assets/04e48750-eaec-42c2-b711-713899e54c63)


# results

![image](https://github.com/user-attachments/assets/55c6f16f-2539-4b8f-ba63-2dfb940a08e6)



